# Description
This page contains instructions for running this Snakemake workflow on your Slurm High Performance Computing Cluster. The pipeline is used to process single or paired-end reads and generates a raw gene and transcript-level count matrix that can be used for differential gene expression analysis (implementation of differential gene expression and enrichment analysis steps are currently underway). Additional outputs include trimmed fastq files, `.bam`, qc'd reports, and a count matrix. 
Currently the pipeline is best-suited for Illumina sequencing data. However, read-processing steps can be modified by the user by providing own adapter sequences. 

## Pipeline overview :label:
1. Raw fastq files (.gz compressed or uncompressed) will be checked for quality using `fastqc`.
2. Trimming and adapter removal will be peformed using `fastp`.
3. A user-provided reference genome or transcriptome in FASTA format will be used for generating an index and alignment. Genome indexing and alignment will be performed using `STAR` while transcriptome indexing and mapping will be performed by `salmon`. ⚠️: You do not need to run both to generate `.bam` files. 
4. To generate gene-level counts `featureCounts` from the `subread` package will be used by providing `.bam` generated by either STAR or Salmon and a genome annotation file in the GTF format as inputs.
5. To generate transcript-level counts `Salmon` will be used using the reference transcriptome as an input. Transcript level counts can be converted to gene-level counts using `scripts/salmon_tximport.r` which uses the `bioconductor::tximport` package. 
To perform normalization of the raw counts, differential gene expression analysis, and gene set enrichment analysis, use `scripts/dge_salmon.r` and `scripts/enrich_anal.svp.R` 

## Input :inbox_tray:
- Paired-end reads or single-end reads. In case of paired data, both forward (`fq_1`) and reverse (`fq_2`) should be present in the `workflow/input` folder. Alternatevly a .TSV file containing sample IDs and paths to reads can be provided (under development).
- Genome or transcriptome reference in FASTA format must be provided in the `workflow/resources` folder. For example, the UCSC human reference genome build 38 can be downloaded using `wget` from [NCBI FTP](https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_full_analysis_set.fna.gz) Corrosponding gene annotations for this reference can also be downloaded in the [GTF format](https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gtf.gz). Transcriptome from the same build can be also be downloaded from the latest [Hg38 Refseq FTP](https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_rna.fna.gz).

## Output :outbox_tray:
- From the Rule 1 Fastqc run, an `.html` and a `gz` compressed `.fastq.gz` summary files should be produced detailing sequencing quality report
- Rule 2 Fastp will produced forward and reverse `.trim_1.fastq.gz` and `.trim_2.fastq.gz` post-contamination and adapter-removed trimmed reads
- Indexing of genome using STAR will generate `SA`, `SAindex`. and several `.tab` files. ⚠️: Index should match build of the reference genome fasta. If Salmon is used `.bin` files should be produced.
- STAR and Salmon alignment step will generate `.bam` for read counting. In case of STAR, sorting `samtools` will be used to generate final `.bam` that are `SortedbyCoordinate`
- Featurecounts will generate an aggregated count matrix `{project_id}_counts.txt`. If Salmon is used for transcript-level quantification, a sample-level `quant.sf` will be generated. These can be aggregated into a single matrix using `scripts/salmon_quant_concat.sh`

## Before Proceeding :raised_hand:
The use of this pipeline requires basic knowledge in Unix Shell and R programming languages for command line execution and manipulation of DGE scripts consisting of bioconductor packages for DGE analysis. Foundational [bash](https://swcarpentry.github.io/shell-novice/) and [R](https://datacarpentry.org/genomics-r-intro/) programming can be studied through Software Carpentery Foundation and Data Carpentery.
User should also be familiar with high performance computing job submission and SLURM workload manager. Refer to the [SLURM documentation](https://slurm.schedmd.com/documentation.html) to get started or contact your local institutional IT adminstrator for guidance.  

## Installation on HPC (SLURM) :battery:
- Requires `conda` to be installed in your home directory. Suggested Miniconda build for running this workflow: [Miniconda3-latest-Linux-x86_64.sh](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) 
  -   On your login node run ```bash Miniconda3-latest-Linux-x86_64.sh```
 - Once conda has been installed, clone this repository into your `/scratch` or `/work` directory using `git clone https://github.com/svpipaliya/bulk-transcriptomics.git`
   -  Navigate to the `snakemake` directory and generate a new RNASEQ conda environment using `env/rnaseq.yaml`
   -  The specific command to install is: ```conda env create --name rnaseq -f env/rnaseq.yaml```. Alternatively, you can use `mamba` in case there are freezes during conda solves.
   -  Activate your environment using ```source activate rnaseq``` or ```conda activate rnaseq```

## Dry-run and DAG :world_map:
- Once all the inputs are placed in the appropriate folder, you can run a dry-run of the steps to ensure the workflow setup is correct
-   To run a dry-run and generting a diacyclic graph of all of the rules, use the following parameter:  `snakemake --dryrun` and `snakemake --rulegraph | dot -tpdf > dag.pdf`

## Cluster submission of the real run :white_check_mark:
IP

